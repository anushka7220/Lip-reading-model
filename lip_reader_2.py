# -*- coding: utf-8 -*-
"""lip reader 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bChocgBNcc7_FMi7BR58H3UkbP8LiaaG

# 0. Install and Import Dependencies
"""

!pip list

!pip install opencv-python matplotlib imageio gdown tensorflow

import os
import cv2
import tensorflow as tf
import numpy as np
from typing import List
from matplotlib import pyplot as plt
import imageio

tf.config.list_physical_devices('GPU')

physical_devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    pass

"""# 1. Build Data Loading Functions"""

import os

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the path to the directory containing your data
data_dir = '/content/drive/MyDrive/data/alignments/s1'

# Check the files in the data directory
print("Files in the data directory:")
for filename in os.listdir(data_dir):
    print(filename)

def load_video(path:str) -> List[float]:

    cap = cv2.VideoCapture(path)
    frames = []
    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):
        ret, frame = cap.read()
        frame = tf.image.rgb_to_grayscale(frame)
        frames.append(frame[190:236,80:220,:])
    cap.release()

    mean = tf.math.reduce_mean(frames)
    std = tf.math.reduce_std(tf.cast(frames, tf.float32))
    return tf.cast((frames - mean), tf.float32) / std

vocab = [x for x in "abcdefghijklmnopqrstuvwxyz'?!123456789 "]

char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="")
num_to_char = tf.keras.layers.StringLookup(
    vocabulary=char_to_num.get_vocabulary(), oov_token="", invert=True
)

print(
    f"The vocabulary is: {char_to_num.get_vocabulary()} "
    f"(size ={char_to_num.vocabulary_size()})"
)

char_to_num.get_vocabulary()

char_to_num(['n','i','c','k'])

num_to_char([14,  9,  3, 11])

def load_alignments(path:str) -> List[str]:
    with open(path, 'r') as f:
        lines = f.readlines()
    tokens = []
    for line in lines:
        line = line.split()
        if line[2] != 'sil':
            tokens = [*tokens,' ',line[2]]
    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]

def load_data(path: tf.Tensor):
    path = tf.convert_to_tensor(path).numpy().decode('utf-8')  # Convert to string
    file_name = os.path.splitext(os.path.basename(path))[0]
    video_path = os.path.join('/content/drive/MyDrive/data/s1', f'{file_name}.mpg')
    alignment_path = os.path.join('/content/drive/MyDrive/data/alignments/s1', f'{file_name}.align')
    frames = load_video(video_path)
    alignments = load_alignments(alignment_path)

    return frames, alignments

test_path = './content/drive/MyDrive/data/s1/bbal6n.mpg'

frames, alignments = load_data(test_path)

plt.imshow(frames[40])

alignments

tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])

def mappable_function(path:str) ->List[str]:
    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))
    return result

"""# 2. Create Data Pipeline"""

from matplotlib import pyplot as plt

data = tf.data.Dataset.list_files('/content/drive/MyDrive/data/s1/*.mpg')
data = data.shuffle(500, reshuffle_each_iteration=False)
data = data.map(mappable_function)
data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))
data = data.prefetch(tf.data.AUTOTUNE)
# Added for split
train = data.take(450)
test = data.skip(450)

len(test)

frames, alignments = load_data(test_path)

len(frames)

sample = data.as_numpy_iterator()

val = sample.next(); val[0]

import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Function to update each frame in the animation
def update(frame):
    plt.imshow(frame)
    plt.axis('off')

# Create a figure and axis
fig, ax = plt.subplots()

# Create the animation
anim = FuncAnimation(fig, update, frames=val[0][0], interval=100)

# Save the animation as a GIF
anim.save('./animation.gif', writer='pillow', fps=10)

# 0:videos, 0: 1st video out of the batch,  0: return the first frame in the video
plt.imshow(val[0][0][35])

tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])

"""# 3. Design the Deep Neural Network"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler

data.as_numpy_iterator().next()[0][0].shape

model = Sequential()
model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(75, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(TimeDistributed(Flatten()))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))

model.summary()

5*17*75

yhat = model.predict(val[0])

tf.strings.reduce_join([num_to_char(x) for x in tf.argmax(yhat[0],axis=1)])

tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[0]])

model.input_shape

model.output_shape

"""# 4. Setup Training Options and Train"""

def scheduler(epoch, lr):
    if epoch < 30:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss

class ProduceExample(tf.keras.callbacks.Callback):
    def __init__(self, dataset) -> None:
        self.dataset = dataset.as_numpy_iterator()

    def on_epoch_end(self, epoch, logs=None) -> None:
        data = self.dataset.next()
        yhat = self.model.predict(data[0])
        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()
        for x in range(len(yhat)):
            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))
            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))
            print('~'*100)

model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)

if len(data) == 0:
    raise ValueError("Error: Dataset is empty.")

# Split dataset into train and test
train_size = int(0.9 * len(data))
if train_size == 0:
    raise ValueError("Error: Dataset is too small for splitting.")
train_data = data.take(train_size)
test_data = data.skip(train_size)

# Verify dataset sizes
print("Train dataset size:", len(train_data))
print("Test dataset size:", len(test_data))

# Check if train_data and test_data contain any elements
if len(train_data) == 0 or len(test_data) == 0:
    raise ValueError("Error: Train or test dataset is empty.")
else:
    # Define callbacks
    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('checkpoint', monitor='loss', save_weights_only=True)
    schedule_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
    example_callback = ProduceExample(test_data)

    # Train the model
    model.fit(train_data, validation_data=test_data, epochs=100, callbacks=[checkpoint_callback, schedule_callback, example_callback])

"""# 5. Make a Prediction"""

import gdown
import zipfile

url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'
output = 'checkpoints.zip'

# Download the file
gdown.download(url, output, quiet=False)

# Extract the downloaded zip file
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall('models')

import tensorflow as tf

# Define the optimizer as a legacy optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

# Compile the model using the optimizer
model.compile(optimizer=optimizer, loss=CTCLoss)

# Initialize the model's variables
model.build((None, 75, 46, 140, 1))

# Create a Checkpoint object
checkpoint = tf.train.Checkpoint(model=model)

# Restore the weights
checkpoint.restore('models/checkpoint')

# Now the model weights are loaded

test_data = test.as_numpy_iterator()

# Iterate over the dataset
for sample in test_data:
    # Extract the target labels or annotations from the sample
    target_labels = sample[1]
    # Process the target labels as needed
    print('~'*100, 'REAL TEXT')
    for sentence in target_labels:
        print(tf.strings.reduce_join([num_to_char(word) for word in sentence]))

# Convert input_length tensor to int32 data type
input_length = tf.cast(input_length, tf.int32)

# Pass logits to the CTC decoder
decoded, log_prob = tf.nn.ctc_greedy_decoder(
    inputs=tf.transpose(logits, perm=[1, 0, 2]),  # Transpose if needed
    sequence_length=input_length,  # Ensure input_length is correctly specified and of int32 data type
    merge_repeated=merge_repeated  # Specify merge_repeated parameter
)

print('~'*100, 'PREDICTIONS')
for sentence in decoded:
    prediction = tf.sparse.to_dense(sentence).numpy()
    decoded_prediction = tf.strings.reduce_join([num_to_char(word) for word in prediction]).numpy().decode('utf-8')
    print(decoded_prediction)

"""# Test on a Video"""

sample = load_data(tf.convert_to_tensor('./content/drive/MyDrive/data/s1/bbal7s.mpg'))

import tensorflow as tf

# Suppress TensorFlow warnings
tf.get_logger().setLevel('ERROR')

print('~'*100, 'REAL TEXT')
real_text = [tf.strings.reduce_join([num_to_char(word) for word in sentence]).numpy().decode('utf-8') for sentence in [sample[1]]]
print(real_text)

yhat = model.predict(tf.expand_dims(sample[0], axis=0))
decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()

print('~'*100, 'PREDICTIONS')
decoded_predictions = [tf.strings.reduce_join([num_to_char(word) for word in sentence]).numpy().decode('utf-8') for sentence in decoded]
print(decoded_predictions)

